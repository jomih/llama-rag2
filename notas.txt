Para problema de certificados, si se usa pipenv:
    export REQUESTS_CA_BUNDLE=./certs/anaconda.org.pem

O si se usa conda:
    conda install -c conda-forge scikit-learn

Esta app Flask combina Llama3.2 con RAG para subir PDFs y conversar con ellos.
    - LLM: llama3.2
    - embeddings: chroma/onnx_models/all-MiniLM-L6-v2/onnx.tar.gz
    - vectorDB: Chroma (https://docs.trychroma.com/guides#adding-data-to-a-collection)
Para lanzar la app:
    - python app.py (funciona sin histórico)
    - python apphistory.py (funciona con historico)

APIs:
 - POST /ai: pregunta generica para LLM (no RAG)
 - POST /ask_text: pregunta sobre documentos cargados (RAG)
 - GET /list_db: listado de documentos cargados
 - POST /add_text: carga de un TXT en concreto. Los .txt tienen que estar tagged (usar script doc_tagging.py)

NOTAS:
    - embedding es el hecho de desgranar el documento (o el texto, o la frase, o el audio ...) y transformarlo en una representacion
    vectorial. Es el modo de transformar datos complejos en vectores numericos
    - una vez representados los datos en vectores, es necesario usar un "vector DB". Ejemplos son: Chromadb, qdrant, ...
    - indexing es el hecho de ordenar la representacion vectorial para optimizar la busqueda posterior de similitudes

Mejoras:
    - borrado de base de datos
    - ingestión de un conjunto de ficheros
    - que el doc_tagging se ejecute automaticamente desde aqui 
    - UI html 
    - pasar respuesta a LLM para que éste la haga bonita